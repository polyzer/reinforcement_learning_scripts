{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://lilianweng.github.io/lil-log/](https://lilianweng.github.io/lil-log/)  \n",
    "[Columbia RL in Robotics](http://www.cs.columbia.edu/~bchen/RL_in_Robotics.pdf)  \n",
    "[PathMind Deep Reinforcement Learning](https://pathmind.com/wiki/deep-reinforcement-learning)\n",
    "\n",
    "[Crowd simulation by deep reinforcement learning](https://dl.acm.org/doi/pdf/10.1145/3274247.3274510?download=true)  \n",
    "\n",
    "\n",
    "Libs:  \n",
    "[garage](https://github.com/rlworkgroup/garage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key concepts:  \n",
    "The agent is acting in an **environment**.  \n",
    "How the environment reacts to certain actions is defined by a **model** which we may or may not know.  \n",
    "The agent can stay in one of many **states** ($s \\in S$) of the environment, and choose to take one of many **actions** ($a \\in A$) to switch from one state to another. Which state the agent will arrive in is decided by transition probabilities between states (P). Once an action is taken, the environment delivers a reward ($r \\in R$) as feedback.\n",
    "\n",
    "The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:\n",
    "* **Know the model**: planning with perfect information; do **model-based RL**. When we fully know the environment, we can find the optimal solution by Dynamic Programming (DP). Do you still remember “longest increasing subsequence” or “traveling salesmen problem” from your Algorithms 101 class? LOL. This is not the focus of this post though.\n",
    "* **Does not know the model**: learning with incomplete information; do **model-free RL** or try to learn the model explicitly as part of the algorithm. Most of the following content serves the scenarios when the model is unknown.\n",
    "\n",
    "The agent’s **policy** $ \\pi (s)$ provides the guideline on what is the optimal action to take in a certain state **with the goal to maximize the total rewards**. Each state is associated with a **value** function $V(s)$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. In other words, the value function quantifies how good a state is. **Both policy and value functions** are what we try to learn in reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: Transition and Reward\n",
    "The model is a descriptor of the environment. With the model, we can learn or infer how the environment would interact with and provide feedback to the agent. The model has two major parts, transition probability function P and reward function $ \\mathbb{R}$. \n",
    "( комментарий мой: \\usepackage{amsfonts} - пакет, необходимый для этой команды. Возможно здесь он уже подключен.)\n",
    "    Далее: $P$ - как обычная формула. Целую! Ты мой самый лучший! Открыла во вкладке полезную книгу Львовского. )))))  \n",
    "Let’s say when we are in state s, we decide to take action a to arrive in the next state s’ and obtain reward r. This is known as one transition step, represented by a tuple $(s, a, s^{’}, r)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **policy** defines the learning agent’s way of behaving at a given time. Roughly speaking,\n",
    "a **policy is a mapping from perceived states of the environment to actions to be taken\n",
    "when in those states**. It corresponds to what in psychology would be called a set of\n",
    "stimulus–response rules or associations. In some cases the policy may be a simple function\n",
    "or lookup table, whereas in others it may involve extensive computation such as a search\n",
    "process. The policy is the core of a reinforcement learning agent in the sense that it alone\n",
    "is sufficient to determine behavior. In general, policies may be stochastic, specifying\n",
    "probabilities for each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **reward signal** defines the goal of a reinforcement learning problem. On each time\n",
    "step, the environment sends to the reinforcement learning agent a single number called\n",
    "the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

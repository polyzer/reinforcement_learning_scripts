{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "[Latex symbols](https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols)\n",
    "[Solving an MDP with Q-Learning from scratch ](https://medium.com/@curiousily/solving-an-mdp-with-q-learning-from-scratch-deep-reinforcement-learning-for-hackers-part-1-45d1d360c120)\n",
    "Rewards:  \n",
    "$R_t = r_t + r_{t+1} + ... + r_n$\n",
    "\n",
    "Discounted reward:\n",
    "$R_t = R_t + \\gamma r_{t+1} + ... + \\gamma^{n-t}r_n = r_t + \\gamma R_{t+1}$\n",
    "\n",
    "Agent must choose the action that maximizes the (discounted) future reward at every step.\n",
    "\n",
    "**Value function:**\n",
    "\n",
    "$V^\\pi(s) = E \\sum\\limits_{t \\geq 0} \\gamma^t r_t) \\ \\ \\forall s \\in S$\n",
    "Where: $s$ - state, $pi$ - policy, $V$ - value function.\n",
    "\n",
    "**Optimal value Function**\n",
    "\n",
    "$V^*(s) = \\max\\limits_{\\pi} V^\\pi (s) \\ \\ \\forall s \\in S$\n",
    "\n",
    "There exists an optimal value function that has the highest value for all states.\n",
    "\n",
    "**Q function**\n",
    "\n",
    "There is a relationship between the two optimal functions $V^∗$ and $Q^∗$\n",
    "\n",
    "\n",
    "$V^*(s) = \\max\\limits_{a} Q^*(s) \\ \\ \\forall s \\in S$\n",
    "\n",
    "That is, the maximum expected total reward when starting at s is the maximum of Q∗(s, a) over all possible actions.\n",
    "\n",
    "\n",
    "Using Q∗(s, a) we can extract the optimal policy π∗ by choosing the action aa that gives maximum reward Q∗(s, a) for state s. We have:\n",
    "\n",
    "$\\pi^*(s) = \\arg \\max\\limits_{a} Q^* (s) \\ \\ \\forall s \\in S$\n",
    "\n",
    "$Q(s,a) = r + \\gamma \\max\\limits_{a'} Q (s',a')$\n",
    "\n",
    "This equation, known as the Bellman equation, tells us that the maximum future reward is the reward the agent received for entering the current state s plus the maximum future reward for the next state s′. The gist of Q-learning is that we can iteratively approximate Q∗ using the Bellman equation described above.\n",
    "\n",
    "$Q_{t+1}(s_t,a_t) = Q_{t}(s_t,a_t) + \\alpha(r_{t+1} + \\gamma \\max\\limits_{a} Q (s_{t+1},a) -  Q (s_{t+1},a) ) \\ \\ \\forall s \\in S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [Building a Powerful DQN in TensorFlow 2.0 ](https://medium.com/analytics-vidhya/building-a-powerful-dqn-in-tensorflow-2-0-explanation-tutorial-d48ea8f3177a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman equation:  \n",
    "$Q^*(s,a) = r + \\gamma max_{a'}(Q^*(s^{'}, a^{'}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"Implements a standard DDDQN agent\"\"\"\n",
    "    def __init__(self,\n",
    "                 dqn,\n",
    "                 target_dqn,\n",
    "                 replay_buffer,\n",
    "                 n_actions,\n",
    "                 input_shape=(84, 84),\n",
    "                 batch_size=32,\n",
    "                 history_length=4,\n",
    "                 eps_initial=1,\n",
    "                 eps_final=0.1,\n",
    "                 eps_final_frame=0.01,\n",
    "                 eps_evaluation=0.0,\n",
    "                 eps_annealing_frames=1000000,\n",
    "                 replay_buffer_start_size=50000,\n",
    "                 max_frames=25000000,\n",
    "                 use_per=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            dqn: A DQN (returned by the DQN function) to predict moves\n",
    "            target_dqn: A DQN (returned by the DQN function) to predict target-q values.  This can be initialized in the same way as the dqn argument\n",
    "            replay_buffer: A ReplayBuffer object for holding all previous experiences\n",
    "            n_actions: Number of possible actions for the given environment\n",
    "            input_shape: Tuple/list describing the shape of the pre-processed environment\n",
    "            batch_size: Number of samples to draw from the replay memory every updating session\n",
    "            history_length: Number of historical frames available to the agent\n",
    "            eps_initial: Initial epsilon value.\n",
    "            eps_final: The \"half-way\" epsilon value.  The epsilon value decreases more slowly after this\n",
    "            eps_final_frame: The final epsilon value\n",
    "            eps_evaluation: The epsilon value used during evaluation\n",
    "            eps_annealing_frames: Number of frames during which epsilon will be annealed to eps_final, then eps_final_frame\n",
    "            replay_buffer_start_size: Size of replay buffer before beginning to learn (after this many frames, epsilon is decreased more slowly)\n",
    "            max_frames: Number of total frames the agent will be trained for\n",
    "            use_per: Use PER instead of classic experience replay\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.input_shape = input_shape\n",
    "        self.history_length = history_length\n",
    "\n",
    "        # Memory information\n",
    "        self.replay_buffer_start_size = replay_buffer_start_size\n",
    "        self.max_frames = max_frames\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.use_per = use_per\n",
    "\n",
    "        # Epsilon information\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "\n",
    "        # Slopes and intercepts for exploration decrease\n",
    "        # (Credit to Fabio M. Graetz for this and calculating epsilon based on frame number)\n",
    "        self.slope = -(self.eps_initial - self.eps_final) / self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_buffer_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame) / (self.max_frames - self.eps_annealing_frames - self.replay_buffer_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "\n",
    "        # DQN\n",
    "        self.DQN = dqn\n",
    "        self.target_dqn = target_dqn\n",
    "\n",
    "    def calc_epsilon(self, frame_number, evaluation=False):\n",
    "        \"\"\"Get the appropriate epsilon value from a given frame number\n",
    "        Arguments:\n",
    "            frame_number: Global frame number (used for epsilon)\n",
    "            evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
    "        Returns:\n",
    "            The appropriate epsilon value\n",
    "        \"\"\"\n",
    "        if evaluation:\n",
    "            return self.eps_evaluation\n",
    "        elif frame_number < self.replay_buffer_start_size:\n",
    "            return self.eps_initial\n",
    "        elif frame_number >= self.replay_buffer_start_size and frame_number < self.replay_buffer_start_size + self.eps_annealing_frames:\n",
    "            return self.slope*frame_number + self.intercept\n",
    "        elif frame_number >= self.replay_buffer_start_size + self.eps_annealing_frames:\n",
    "            return self.slope_2*frame_number + self.intercept_2\n",
    "\n",
    "    def get_action(self, frame_number, state, evaluation=False):\n",
    "        \"\"\"Query the DQN for an action given a state\n",
    "        Arguments:\n",
    "            frame_number: Global frame number (used for epsilon)\n",
    "            state: State to give an action for\n",
    "            evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
    "        Returns:\n",
    "            An integer as the predicted move\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate epsilon based on the frame number\n",
    "        eps = self.calc_epsilon(frame_number, evaluation)\n",
    "\n",
    "        # With chance epsilon, take a random action\n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "\n",
    "        # Otherwise, query the DQN for an action\n",
    "        q_vals = self.DQN.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))[0]\n",
    "        return q_vals.argmax()\n",
    "\n",
    "    def get_intermediate_representation(self, state, layer_names=None, stack_state=True):\n",
    "        \"\"\"\n",
    "        Get the output of a hidden layer inside the model.  This will be/is used for visualizing model\n",
    "        Arguments:\n",
    "            state: The input to the model to get outputs for hidden layers from\n",
    "            layer_names: Names of the layers to get outputs from.  This can be a list of multiple names, or a single name\n",
    "            stack_state: Stack `state` four times so the model can take input on a single (84, 84, 1) frame\n",
    "        Returns:\n",
    "            Outputs to the hidden layers specified, in the order they were specified.\n",
    "        \"\"\"\n",
    "        # Prepare list of layers\n",
    "        if isinstance(layer_names, list) or isinstance(layer_names, tuple):\n",
    "            layers = [self.DQN.get_layer(name=layer_name).output for layer_name in layer_names]\n",
    "        else:\n",
    "            layers = self.DQN.get_layer(name=layer_names).output\n",
    "\n",
    "        # Model for getting intermediate output\n",
    "        temp_model = tf.keras.Model(self.DQN.inputs, layers)\n",
    "\n",
    "        # Stack state 4 times\n",
    "        if stack_state:\n",
    "            if len(state.shape) == 2:\n",
    "                state = state[:, :, np.newaxis]\n",
    "            state = np.repeat(state, self.history_length, axis=2)\n",
    "\n",
    "        # Put it all together\n",
    "        return temp_model.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target Q network\"\"\"\n",
    "        self.target_dqn.set_weights(self.DQN.get_weights())\n",
    "\n",
    "    def add_experience(self, action, frame, reward, terminal, clip_reward=True):\n",
    "        \"\"\"Wrapper function for adding an experience to the Agent's replay buffer\"\"\"\n",
    "        self.replay_buffer.add_experience(action, frame, reward, terminal, clip_reward)\n",
    "\n",
    "    def learn(self, batch_size, gamma, frame_number, priority_scale=1.0):\n",
    "        \"\"\"Sample a batch and use it to improve the DQN\n",
    "        Arguments:\n",
    "            batch_size: How many samples to draw for an update\n",
    "            gamma: Reward discount\n",
    "            frame_number: Global frame number (used for calculating importances)\n",
    "            priority_scale: How much to weight priorities when sampling the replay buffer. 0 = completely random, 1 = completely based on priority\n",
    "        Returns:\n",
    "            The loss between the predicted and target Q as a float\n",
    "        \"\"\"\n",
    "\n",
    "        if self.use_per:\n",
    "            (states, actions, rewards, new_states, terminal_flags), importance, indices = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
    "            importance = importance ** (1-self.calc_epsilon(frame_number))\n",
    "        else:\n",
    "            states, actions, rewards, new_states, terminal_flags = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
    "\n",
    "        # Main DQN estimates best action in new states\n",
    "        arg_q_max = self.DQN.predict(new_states).argmax(axis=1)\n",
    "\n",
    "        # Target DQN estimates q-vals for new states\n",
    "        future_q_vals = self.target_dqn.predict(new_states)\n",
    "        double_q = future_q_vals[range(batch_size), arg_q_max]\n",
    "\n",
    "        # Calculate targets (bellman equation)\n",
    "        target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
    "\n",
    "        # Use targets to calculate loss (and use loss to calculate gradients)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.DQN(states)\n",
    "\n",
    "            one_hot_actions = tf.keras.utils.to_categorical(actions, self.n_actions, dtype=np.float32)  # using tf.one_hot causes strange errors\n",
    "            Q = tf.reduce_sum(tf.multiply(q_values, one_hot_actions), axis=1)\n",
    "\n",
    "            error = Q - target_q\n",
    "            loss = tf.keras.losses.Huber()(target_q, Q)\n",
    "\n",
    "            if self.use_per:\n",
    "                # Multiply the loss by importance, so that the gradient is also scaled.\n",
    "                # The importance scale reduces bias against situataions that are sampled\n",
    "                # more frequently.\n",
    "                loss = tf.reduce_mean(loss * importance)\n",
    "\n",
    "        model_gradients = tape.gradient(loss, self.DQN.trainable_variables)\n",
    "        self.DQN.optimizer.apply_gradients(zip(model_gradients, self.DQN.trainable_variables))\n",
    "\n",
    "        if self.use_per:\n",
    "            self.replay_buffer.set_priorities(indices, error)\n",
    "\n",
    "        return float(loss.numpy()), error\n",
    "\n",
    "    def save(self, folder_name, **kwargs):\n",
    "        \"\"\"Saves the Agent and all corresponding properties into a folder\n",
    "        Arguments:\n",
    "            folder_name: Folder in which to save the Agent\n",
    "            **kwargs: Agent.save will also save any keyword arguments passed.  This is used for saving the frame_number\n",
    "        \"\"\"\n",
    "\n",
    "        # Create the folder for saving the agent\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        # Save DQN and target DQN\n",
    "        self.DQN.save(folder_name + '/dqn.h5')\n",
    "        self.target_dqn.save(folder_name + '/target_dqn.h5')\n",
    "\n",
    "        # Save replay buffer\n",
    "        self.replay_buffer.save(folder_name + '/replay-buffer')\n",
    "\n",
    "        # Save meta\n",
    "        with open(folder_name + '/meta.json', 'w+') as f:\n",
    "            f.write(json.dumps({**{'buff_count': self.replay_buffer.count, 'buff_curr': self.replay_buffer.current}, **kwargs}))  # save replay_buffer information and any other information\n",
    "\n",
    "    def load(self, folder_name, load_replay_buffer=True):\n",
    "        \"\"\"Load a previously saved Agent from a folder\n",
    "        Arguments:\n",
    "            folder_name: Folder from which to load the Agent\n",
    "        Returns:\n",
    "            All other saved attributes, e.g., frame number\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.isdir(folder_name):\n",
    "            raise ValueError(f'{folder_name} is not a valid directory')\n",
    "\n",
    "        # Load DQNs\n",
    "        self.DQN = tf.keras.models.load_model(folder_name + '/dqn.h5')\n",
    "        self.target_dqn = tf.keras.models.load_model(folder_name + '/target_dqn.h5')\n",
    "        self.optimizer = self.DQN.optimizer\n",
    "\n",
    "        # Load replay buffer\n",
    "        if load_replay_buffer:\n",
    "            self.replay_buffer.load(folder_name + '/replay-buffer')\n",
    "\n",
    "        # Load meta\n",
    "        with open(folder_name + '/meta.json', 'r') as f:\n",
    "            meta = json.load(f)\n",
    "\n",
    "        if load_replay_buffer:\n",
    "            self.replay_buffer.count = meta['buff_count']\n",
    "            self.replay_buffer.current = meta['buff_curr']\n",
    "\n",
    "        del meta['buff_count'], meta['buff_curr']  # we don't want to return this information\n",
    "        return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.layers import (Add, Conv2D, Dense, Flatten, Input,\n",
    "                                     Lambda, Subtract)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def build_q_network(n_actions, learning_rate=0.00001, input_shape=(84, 84), history_length=4):\n",
    "    \"\"\"Builds a dueling DQN as a Keras model\n",
    "    Arguments:\n",
    "        n_actions: Number of possible action the agent can take\n",
    "        learning_rate: Learning rate\n",
    "        input_shape: Shape of the preprocessed frame the model sees\n",
    "        history_length: Number of historical frames the agent can see\n",
    "    Returns:\n",
    "        A compiled Keras model\n",
    "    \"\"\"\n",
    "    model_input = Input(shape=(input_shape[0], input_shape[1], history_length))\n",
    "    x = Lambda(lambda layer: layer / 255)(model_input)  # normalize by 255\n",
    "\n",
    "    x = Conv2D(32, (8, 8), strides=4, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=2, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(64, (3, 3), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(1024, (7, 7), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "\n",
    "    # Split into value and advantage streams\n",
    "    val_stream, adv_stream = Lambda(lambda w: tf.split(w, 2, 3))(x)  # custom splitting layer\n",
    "\n",
    "    val_stream = Flatten()(val_stream)\n",
    "    val = Dense(1, kernel_initializer=VarianceScaling(scale=2.))(val_stream)\n",
    "\n",
    "    adv_stream = Flatten()(adv_stream)\n",
    "    adv = Dense(n_actions, kernel_initializer=VarianceScaling(scale=2.))(adv_stream)\n",
    "\n",
    "    # Combine streams into Q-Values\n",
    "    reduce_mean = Lambda(lambda w: tf.reduce_mean(w, axis=1, keepdims=True))  # custom layer for reduce mean\n",
    "\n",
    "    q_vals = Add()([val, Subtract()([adv, reduce_mean(adv)])])\n",
    "\n",
    "    # Build model\n",
    "    model = Model(model_input, q_vals)\n",
    "    model.compile(Adam(learning_rate), loss=tf.keras.losses.Huber())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'process_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6b12d90ede8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# This is the process_frame function we implemented earlier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprocess_frame\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'process_frame'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# This is the process_frame function we implemented earlier\n",
    "from process_frame import process_frame\n",
    "\n",
    "\n",
    "class GameWrapper:\n",
    "    \"\"\"Wrapper for the environment provided by Gym\"\"\"\n",
    "    def __init__(self, env_name, no_op_steps=10, history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.history_length = 4\n",
    "\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "\n",
    "    def reset(self, evaluation=False):\n",
    "        \"\"\"Resets the environment\n",
    "        Arguments:\n",
    "            evaluation: Set to True when the agent is being evaluated. Takes a random number of no-op steps if True.\n",
    "        \"\"\"\n",
    "\n",
    "        self.frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "\n",
    "        # If evaluating, take a random number of no-op steps.\n",
    "        # This adds an element of randomness, so that the each\n",
    "        # evaluation is slightly different.\n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(0, self.no_op_steps)):\n",
    "                self.env.step(1)\n",
    "\n",
    "        # For the initial state, we stack the first frame four times\n",
    "        self.state = np.repeat(process_frame(self.frame), self.history_length, axis=2)\n",
    "\n",
    "    def step(self, action, render_mode=None):\n",
    "        \"\"\"Performs an action and observes the result\n",
    "        Arguments:\n",
    "            action: An integer describe action the agent chose\n",
    "            render_mode: None doesn't render anything, 'human' renders the screen in a new window, 'rgb_array' returns an np.array with rgb values\n",
    "        Returns:\n",
    "            processed_frame: The processed new frame as a result of that action\n",
    "            reward: The reward for taking that action\n",
    "            terminal: Whether the game has ended\n",
    "            life_lost: Whether a life has been lost\n",
    "            new_frame: The raw new frame as a result of that action\n",
    "            If render_mode is set to 'rgb_array' this also returns the rendered rgb_array\n",
    "        \"\"\"\n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "\n",
    "        # In the commonly ignored 'info' or 'meta' data returned by env.step\n",
    "        # we can get information such as the number of lives the agent has.\n",
    "\n",
    "        # We use this here to find out when the agent loses a life, and\n",
    "        # if so, we set life_lost to True.\n",
    "\n",
    "        # We use life_lost to force the agent to start the game\n",
    "        # and not sit around doing nothing.\n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            life_lost = True\n",
    "        else:\n",
    "            life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "\n",
    "        processed_frame = process_frame(new_frame)\n",
    "        self.state = np.append(self.state[:, :, 1:], processed_frame, axis=2)\n",
    "\n",
    "        if render_mode == 'rgb_array':\n",
    "            return processed_frame, reward, terminal, life_lost, self.env.render(render_mode)\n",
    "        elif render_mode == 'human':\n",
    "            self.env.render()\n",
    "\n",
    "        return processed_frame, reward, terminal, life_lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
